%!TEX root = main.tex

We have compared {\ostrich}+ with the state-of-the-art solvers on a wide range of benchmarks.  We will discuss the benchmarks in Section~\ref{sec:bench}, then describe the details of the experiments as well as the experiment results in Section~\ref{sec:exp-res}.
% and SLENT \cite{WC+18}. 

 %
%OSTRICH implements the optimised
%decision procedure for string functions as described in Section 5.1 (i.e. using distributivity of regular
%constraints across pre-images of functions) and has built-in support for concatenation, reverse, FFT,
%and replaceAll. Moreover, since the optimisation only requires that string operations are functional,
%we can also support additional functions that satisfy RegInvRel, such as replacee which replaces
%only the first (leftmost and longest) match of a regular expression. OSTRICH is extensible and new
%string functions can be added quite simply (Section 6.3).
%Our implementation adds a new theory solver for conjunctive formulas representing path
%feasibility problems to Princess (Section 6.1). This means that we support disjunction as well as
%conjunction in formulas, as long as every conjunction of literals fed to the theory solver corresponds
%to a path feasibility problem. OSTRICH also implements a number of optimisations to efficiently
%compute pre-images of relevant functions (Section 6.2). OSTRICH is entirely written in Scala and is
%open-source. We report on our experiments with OSTRICH in Section 6.4. The tool is available on
%GitHub6.  


\subsection{Benchmarks}\label{sec:bench}
 
%In
%particular, we compared  with CVC4 1.6 \cite{}, SLOTH \cite{},
%and Z3 configured to use the Z3-str3 string solver \cite{}. 

%We considered several sets
%of benchmarks which are described in the next sub-section. The results are given in Section 6.4.2.
%
%
%In [Holík et al. 2018] SLOTH was compared with S3P [Trinh et al. 2016] where inconsistent
%behaviour was reported. We contacted the S3P authors who report that the current code is unsupported;
%moreover, S3P is being integrated with Z3. Hence, we do not compare with this tool.
%
%
%The first set of benchmarks we call \textbf{Transducer}. It combines the benchmark
%sets of Stranger [Yu et al. 2010] and the mutation XSS benchmarks of [Lin and Barceló 2016]. The
%first (sub-)set appeared in [Holík et al. 2018] and contains instances manually derived from PHP
%programs taken from the website of Stranger [Yu et al. 2010]. It contains 10 formulae (5 sat, 5
%unsat) each testing for the absence of the vulnerability pattern .*<script.* in the program output.
%These formulae contain between 7 and 42 variables, with an average of 21. The number of atomic
%constraints ranges between 7 and 38 and averages 18. These examples use disjunction, conjunction,
%regular constraints, and concatenation, replaceAll. They also contain several one-way functional
%transducers (defined in SMTLIB in [Holík et al. 2018]) encoding functions such as addslashes and
%trim used by the programs. Note that transducers have been known for some time to be a good
%framework for specifying sanitisers and browser transductions (e.g., see the works by Minamide,
%Veanes, Saxena, and others [D’Antoni and Veanes 2013; Hooimeijer et al. 2011; Minamide 2005;
%Weinberger et al. 2011]), and a library of transducer specifications for such functions is available
%(e.g. see the language BEK [Hooimeijer et al. 2011]).
%
%The second (sub-)set was used by [Holík et al. 2018; Lin and Barceló 2016] and consists of 8
%formulae taken from [Kern 2014; Lin and Barceló 2016]. These examples explore mutation XSS
%vulnerabilities in JavaScript programs. They contain between 9 and 12 variables, averaging 9.75, and
%9-13 atomic constraints, with an average of 10.5. They use conjunctions, regular constraints, concatenation,
%replaceAll, and transducers providing functions such as htmlEscape and escapeString.
%Our next set of benchmarks, SLOG, came from the SLOG tool [Wang et al. 2016] and consist of
%3,392 instances. They are derived from the security analysis of real web applications and contain 1-
%211 string variables (average 6.5) and 1-182 atomic formula (average 5.8).We split these benchmarks
%into two sets SLOG (replace) and SLOG (replaceall). Each use conjunction, disjunction, regular
%constraints, and concatenation. The set SLOG (replace) contains 3,391 instances and uses replace.
%SLOG (replaceall) contains 120 instances using the replaceAll operation.
%
%
%Our next set of benchmarks Kaluza is the well-known set of Kaluza benchmarks [Saxena et al.
%2010] restricted to those instances which satisfy our semantic conditions (roughly 80\% of the
%benchmarks). Kaluza contains concatenation, regular constraints, and length constraints, most of
%which admit regular monadic decomposition. There are 37,090 such benchmarks (28 032 sat).
%We also considered the benchmark set of [Chen 2018a,b]. This contains 42 hand-crafted benchmarks
%using regular constraints, concatenation, and replaceAll with variables in both argument
%positions. The benchmarks contain 3-7 string variables and 3-9 atomic constraints.
% 

We consider the following four sets of benchmarks.

The first benchmark suite {\transducerbench+} is derived from the {\transducerbench} benchmark suite of {\ostrich} \cite{CHL+19}.  The {\transducerbench} suite involves seven transducers: toUpper (replacing all lowercase letters with their uppercase ones) and its dual toLower, htmlEscape \cite{htmlEscape} and its dual htmlUnescape, escapeString \cite{escapeString}, addslashes \cite{addslashes}, and trim \cite{trim}. These transducers are collected from Strangers \cite{YABI14} and SLOTH \cite{HJLRV18}, which address strings only. To build up the {\transducerbench+} suite, %is generated from these transducers by 
we encode five desirable properties of transducers, which have been studied in security \cite{BEK}, into our constraint language {\slint}. The encoding we provide makes use of $\charat$ and $\length$. These properties include: idempotence (given $\NFT$, whether $\forall x.\ \NFT(\NFT(x)) = \NFT(x)$), duality (given $\NFT_1$ and $\NFT_2$, whether $\forall x.\ \NFT_2(\NFT_1(x)) = x$), commutativity (given $\NFT_1$ and $\NFT_2$, whether $\forall x.\ \NFT_2(\NFT_1(x)) = \NFT_1(\NFT_2(x))$), and equivalence (given $\NFT_1$ and $\NFT_2$, whether $\forall x.\ \NFT_1(x) = \NFT_2(x)$) problems
%
%\footnote{These problems have been investigated for transducers in \cite{BEK}.} 
%into the path feasibility of {\slint} programs. 
For instance, we encode the non-idempotence of $\NFT$ into the path feasibility of the {\slint} program $y:=\NFT(x); z:=\NFT(y); S_{y \neq z}$, where $y$ and $z$ are two fresh string variables, and $S_{y \neq z}$ is the {\slint} program encoding $y \neq z$, which uses $\length$ and $\charat$ (see Section~\ref{sec:logic} for the details of $S_{y \neq z}$). Moreover, we also include in {\transducerbench+} three {\slint} programs generated from the {\tt urlXssSanitise} function in Example~\ref{exmp:running}, where the transducer $\NFT_{\rm trim}$ is used. In total, we obtain 94 instances for the {\transducerbench+} benchmark suite. 
%\begin{description}
%\item[Idempotence.] For each FFT $\NFT$,  we encode the non-idempotence of $\NFT$, namely deciding whether $\exists x.\ \NFT(\NFT(x)) \neq \NFT(x)$, into the path feasibility of the {\slint} program $y:=\NFT(x); z:=\NFT(y); S_{y \neq z}$, where $y$ and $z$ are two fresh string variables, and $S_{y \neq z}$ is the {\slint} program encoding $y \neq z$, which uses $\length$ and $\charat$ (see Section~\ref{sec:logic} for the details of $S_{y \neq z}$).
%
%\item[Duality.] For each pair of distinct FFTs $\NFT_1$ and $\NFT_2$, we encode the non-duality of $\NFT_1$ and $\NFT_2$, namely deciding whether $\exists x.\ \NFT_2(\NFT_1(x)) \neq x$, into the path feasibility of the {\slint} program $y:=\NFT_1(x); z: = \NFT_2(y); S_{x \neq z}$.
%
%\item[Commutativity.] For each pair of distinct FFTs $\NFT_1$ and $\NFT_2$, we encode the non-commutativity of $\NFT_1$ and $\NFT_2$, namely deciding whether $\exists x.\ \NFT_2(\NFT_1(x)) \neq \NFT_1(\NFT_2(x))$, into the path feasibility of the {\slint} program $y:=\NFT_1(x); z: = \NFT_2(y); y':= \NFT_2(x); z': = \NFT_1(y'); S_{z \neq z'}$.
%
%\item[Equivalence.] For each pair of distinct FFTs $\NFT_1$ and $\NFT_2$, we encode the non-equivalence of $\NFT_1$ and $\NFT_2$, namely deciding whether $\exists x.\ \NFT_1(x) \neq \NFT_2(x)$, into the path feasibility of the {\slint} program $y:=\NFT_1(x); z: = \NFT_2(x); S_{y \neq z}$.
%
%\end{description}

The second benchmark suite {\slogbench} is adapted from the SLOG benchmark suite used by the SLOG tool~\cite{fang-yu-circuits} and contains 3,511 instances. 
%The SLOG are derived from the security analysis of real web applications and contain 1-211 string variables (average 6.5) and 1-182 atomic formula (average 5.8).
Note that the SLOG benchmark suite addresses strings only. We  obtain the {\slogbench} suite by choosing a string variable $x$ for each instance,
%SLOG benchmark instance ,
%\footnote{A string variable is called the output variable if it occurs in the left-hand side of an assignment statement, but does not appear in the right-hand sides of the assignment statements.} 
%say $x$, 
and add the statement $\ASSERT{\length(x) < 2\ \indexof_{a}(x, 0)}$ for some $a \in \Sigma$.
% and $c \in \Nat$ with $c > 1$.
Following \cite{CHL+19} We further split the {\slogbench} benchmark suite into two sub-suites \slogbenchr\ and \slogbenchra, which comprises 3,391 instances and 120 instances respectively. In addition to the aforementioned $\indexof$ and $\length$ functions, both  \slogbenchr\ and \slogbenchra\ benchmarks use regular constraints and concatenation, the difference is in that {\slogbenchr} benchmarks use the $\replace$ function (replacing the first occurrence), while {\slogbenchra} benchmarks use the $\replaceall$ function (replacing all occurrences).

%Each use conjunction, disjunction, regular constraints, and concatenation.
%The \slogbenchr\ sub-suite contains 3,391 instances \zhilin{the number to be checked}, while \slogbenchra\ sub-suite contains 120 instances \zhilin{the number to be checked}.

The third benchmark suite  {\pyexbench} suite \cite{ReynoldsWBBLT17} comprises 25,421 instances. 
This suite is derived from the PyEx tool, a symbolic execution engine for Python programs. The {\pyexbench} suite was generated by the CVC4 group from four popular Python packages, i.e., httplib2, pip, pymongo, and requests. These instances use regular constraints, concatenation, $\length$, $\substring$, and $\indexof$ functions. Following \cite{ReynoldsWBBLT17}, the {\pyexbench} suite is further divided into the three parts: {\pyextdbench},  {\pyexztbench} and {\pyexzzbench}, comprising 5,569, 8,414 and 11438  instances respectively. 

The fourth benchmark suite {\kaluzabench} is the well-known Kaluza benchmark suite~\cite{Berkeley-JavaScript}.
{\kaluzabench} contains 47,284 instances, which use regular constraints, concatenation, and $\length$ function.





\subsection{Experiments}\label{sec:exp-res}

We compare {\ostrich}+ with {\cvc} \cite{cvc4}, {\zthree} \cite{Z3-str}, and {\zthreetrau} \cite{Z3-trau}, on the aforementioned benchmark suites. In the beginning, we also planned to compare {\ostrich}+ with two other solvers, namely {\trauplus} \cite{AbdullaA+19} and {\slent} \cite{WC+18}, but finally gave up, with some justifications of this decision explained below. 
\begin{itemize}
\item {\trauplus} integrates {\trau} with {\sloth} in order to deal with both finite transducers (including the $\replaceall$ function) and integer constraints. Therefore, at the first sight, we had better compare {\ostrich}+ with {\trau+} on the {\transducerbench}+ benchmark suite. Nevertheless, we failed to run {\trau+} (even after some hard work), possibly due to the fact that {\trau} requires two versions of Z3 to run.
%,  though we were able to compile the source code of \trauplus.  
After some deeper analysis, we realised that {\transducerbench}+ benchmark suite is actually beyond the scope of {\trauplus} since {\trauplus} integrates {\trau} with {\sloth} in a shallow and largely separated way in the sense that it uses {\sloth} to deal with finite transducers and {\trau} to handle integer constraints, while {\sloth} is unable to solve integer constraints and {\trau}  is incapable of dealing with finite transducers.
%
\item {\slent} uses an input format called laut which is different from the widely used smtlib2 format. Therefore, it is hard to compare {\ostrich}+ with {\slent} on {\kaluzabench} and {\pyexbench} benchmark suites. Moreover, {\transducerbench}+ is beyond the scope of {\slent} since it is incapable of dealing with finite transducers.
\end{itemize}

The experiments are executed on a virtual machine with Inter Xeon Silver 4210 2.20GHz and 2.19GHz CPU (2-core) and 8GB main memory, running 64bit Ubuntu 18.04 LTS OS and Java 1.8, with the timeout period set as 30 seconds. The experiment results are summarised in Table~\ref{tab-experiment}.

{\zthreetrau} does not support replace/replaceall.




observation: 1. ostrich+ is the only tool that is capable of solving transducer+ benchmarks. 2. ostrich+ is good at solving unsatisfiable benchmarks, for instance, replaceall, replace, pyextd, pyexz3



%From the experimental results, we can see that \ostrich+ is the only solver that 



\definecolor{Gray}{gray}{0.9}
%\newcolumntype{g}{>{\columncolor{Gray}}c}
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Benchmark suite & Output &  \cvc & \zthree &  \zthreetrau & \ostrich+\\
\hline
\multirow{4}{*}{\transducerbench+(94)} & \cellcolor{Gray} sat &  \cellcolor{Gray}$-$ & \cellcolor{Gray}$-$ & \cellcolor{Gray}$-$ & \cellcolor{Gray}\bf{85}\\
\cline{2-6}
 & unsat &$-$  &$-$ &$-$ &\bf{9}\\
\cline{2-6}
 & \cellcolor{Gray}  u.k. / t.o. / e.x.  &\cellcolor{Gray}$-$    &\cellcolor{Gray}$-$  &\cellcolor{Gray}$-$  &\cellcolor{Gray}0\\
\cline{2-6} 
 & wrong &$-$  & $-$  &$-$ &0 \\
\hline
\multirow{4}{*}{\slogbenchra(120)} & \cellcolor{Gray} sat &  \cellcolor{Gray}\bf{104}  & \cellcolor{Gray}$-$ & \cellcolor{Gray}$-$  &98 \cellcolor{Gray}\\
\cline{2-6}
 & unsat &11  &$-$  &$-$ &\bf{12}\\
\cline{2-6}
 &\cellcolor{Gray} u.k. / t.o. / e.x. & \cellcolor{Gray}5  &\cellcolor{Gray}$-$ &\cellcolor{Gray}$-$ &\cellcolor{Gray}10\\
\cline{2-6}
 & wrong &0 &$-$ &$-$  &0 \\
\hline
\multirow{4}{*}{\slogbenchr(3,391)} & \cellcolor{Gray} sat &  \cellcolor{Gray}\bf{1,309} & \cellcolor{Gray}878 & \cellcolor{Gray}$-$ & \cellcolor{Gray}584 \\
\cline{2-6}
 & unsat & \bf{2,082} & 2,066  &$-$ &\bf{2,082}\\
\cline{2-6}
 &\cellcolor{Gray}  u.k. / t.o. / e.x. & \cellcolor{Gray}0  &  \cellcolor{Gray}447   &  \cellcolor{Gray}$-$ &\cellcolor{Gray}725\\
\cline{2-6}
 & wrong &0 & 0  &$-$ &0\\
\hline
\multirow{4}{*}{\pyextdbench(5,569)} & \cellcolor{Gray} sat & \cellcolor{Gray}4,224 & \cellcolor{Gray}4,068 &  \cellcolor{Gray} \bf{4,266} & \cellcolor{Gray}4,141\\
\cline{2-6}
 & unsat & 1,284 & 1,289 & \bf{1,295} &1,203\\
\cline{2-6}
 &\cellcolor{Gray} u.k. / t.o. / e.x. &\cellcolor{Gray}61 &\cellcolor{Gray}212   &\cellcolor{Gray}8 &\cellcolor{Gray}225\\
\cline{2-6}
 &wrong &0 & 0 &  0 &0 \\
\hline
\multirow{4}{*}{\pyexztbench(8,414)} & \cellcolor{Gray} sat & \cellcolor{Gray}6,346 & \cellcolor{Gray}6,040 & \cellcolor{Gray}\bf{7,003} & \cellcolor{Gray}5,489\\
\cline{2-6}
 & unsat & 1,358  & 1,370  &\bf{1,394} &1,239\\
\cline{2-6}
 & \cellcolor{Gray}u.k. / t.o. / e.x. &\cellcolor{Gray}710 &\cellcolor{Gray}1,004 &\cellcolor{Gray} 17 &\cellcolor{Gray}1,686\\
\cline{2-6}
 & wrong & 0 &0 & 0 &0 \\
\hline
\multirow{4}{*}{\pyexzzbench(11,438)} & \cellcolor{Gray} sat & \cellcolor{Gray} 10,078 & \cellcolor{Gray} 8,804 & \cellcolor{Gray} \bf{10,129} & \cellcolor{Gray}\\
\cline{2-6}
 & unsat & 1,204 & 1,207  &   \bf{1,222} &\\
\cline{2-6}
 &\cellcolor{Gray}  u.k. / t.o. / e.x. &\cellcolor{Gray}156 & \cellcolor{Gray}1,427  &  \cellcolor{Gray} 87 &\cellcolor{Gray} \\
\cline{2-6}
 & wrong &  0 & 0 & 0& \\
\hline
\multirow{4}{*}{\kaluzabench(47,284)} & \cellcolor{Gray} sat &  \cellcolor{Gray} \bf{35,264} & \cellcolor{Gray} 33,438 & \cellcolor{Gray} 34,769 & \cellcolor{Gray}27,962\\
\cline{2-6}
 & unsat & \bf{12,014} &  11,799  &\bf{12,014}  &9,058\\
\cline{2-6}
 &\cellcolor{Gray} u.k. / t.o. / e.x. &\cellcolor{Gray}6 & \cellcolor{Gray}2,047  &\cellcolor{Gray}501 &\cellcolor{Gray}10,264 \\
\cline{2-6}
 & wrong &  0 & 0 &0 &0 \\
\hline
\multirow{2}{*}{Total(76,310)} & \cellcolor{Gray} solved & \cellcolor{Gray}\bf{75,278}  & \cellcolor{Gray}70,959 & \cellcolor{Gray}72,092 & \cellcolor{Gray}\\
\cline{2-6}
 &  unsolved &1,032  & 5,351  & 4,218 &  \\
\hline
\end{tabular}
\end{center}
\caption{Experiment results: u.k.(unknown), t.o.(timeout), e.x.(exception).}
\label{tab-experiment}
\end{table}%